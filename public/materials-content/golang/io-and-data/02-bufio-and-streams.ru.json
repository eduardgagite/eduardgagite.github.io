{"content":"\nЧто делать, если файл весит 10 ГБ, а памяти у вас всего 2 ГБ? Читать его целиком нельзя — программа упадет.\n\nЗдесь на помощь приходят **потоки** (Streams). Идея: мы читаем файл не весь сразу, а маленькими кусочками (буфером), обрабатываем и выбрасываем. В любой момент в памяти находится только текущий кусок.\n\n## Чтение построчно (bufio.Scanner)\n\nПакет `bufio` (Buffered I/O) — главный инструмент для потокового чтения текста.\n\n```go\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"os\"\n)\n\nfunc main() {\n    // 1. Открываем файл (он НЕ загружается в память целиком)\n    file, err := os.Open(\"access.log\")\n    if err != nil {\n        panic(err)\n    }\n    defer file.Close()\n\n    // 2. Создаем сканер\n    scanner := bufio.NewScanner(file)\n\n    // 3. Читаем строка за строкой\n    lineNum := 0\n    for scanner.Scan() {\n        lineNum++\n        line := scanner.Text()\n        fmt.Printf(\"Строка %d: %s\\n\", lineNum, line)\n    }\n\n    // 4. Проверяем, не было ли ошибки при чтении\n    if err := scanner.Err(); err != nil {\n        fmt.Println(\"Ошибка:\", err)\n    }\n}\n```\n\nЭтот код обработает файл **любого** размера — хоть терабайт — потребляя всего пару килобайт памяти.\n\n### Ограничение размера строки\n\nПо умолчанию `Scanner` читает строки до 64 КБ. Если строка длиннее (например, JSON на одну строку), нужно увеличить буфер:\n\n```go\nscanner := bufio.NewScanner(file)\nscanner.Buffer(make([]byte, 1024*1024), 1024*1024) // Буфер на 1 МБ\n```\n\n## Запись через буфер (bufio.Writer)\n\nЗапись по одному символу напрямую в файл — медленно (каждый раз системный вызов). `bufio.Writer` накапливает данные в буфере и пишет на диск большими порциями.\n\n```go\nfile, _ := os.Create(\"output.txt\")\ndefer file.Close()\n\nwriter := bufio.NewWriter(file)\n\nfor i := 0; i < 10000; i++ {\n    writer.WriteString(fmt.Sprintf(\"Строка %d\\n\", i))\n}\n\n// ВАЖНО: Не забудьте Flush! Иначе последние данные останутся в буфере\nwriter.Flush()\n```\n\n## Интерфейсы io.Reader и io.Writer\n\nЭто **фундамент** I/O в Go. Почти всё, что можно прочитать, реализует `io.Reader`. Почти всё, куда можно записать, реализует `io.Writer`.\n\n```go\ntype Reader interface {\n    Read(p []byte) (n int, err error)\n}\n\ntype Writer interface {\n    Write(p []byte) (n int, err error)\n}\n```\n\nПочему это важно? Потому что вы можете написать **одну функцию**, которая работает с любым источником данных.\n\n```go\n// Эта функция не знает, откуда приходят данные\nfunc countLines(r io.Reader) int {\n    scanner := bufio.NewScanner(r)\n    count := 0\n    for scanner.Scan() {\n        count++\n    }\n    return count\n}\n\nfunc main() {\n    // Считаем строки в файле\n    f, _ := os.Open(\"data.txt\")\n    defer f.Close()\n    fmt.Println(countLines(f))\n\n    // Считаем строки в строке (из памяти)\n    s := strings.NewReader(\"строка 1\\nстрока 2\\nстрока 3\")\n    fmt.Println(countLines(s)) // 3\n\n    // Считаем строки из HTTP ответа\n    resp, _ := http.Get(\"https://example.com\")\n    defer resp.Body.Close()\n    fmt.Println(countLines(resp.Body))\n}\n```\n\nОдна и та же функция `countLines` работает с файлом, строкой и HTTP-ответом. Всё благодаря интерфейсу `io.Reader`.\n\n## Итог\n\n1. Для больших файлов — `bufio.Scanner` (построчное чтение без загрузки в память).\n2. Для массовой записи — `bufio.Writer` (буферизация ускоряет запись).\n3. `io.Reader` и `io.Writer` — универсальные интерфейсы. Пишите функции, которые принимают их, а не конкретные типы файлов.\n"}